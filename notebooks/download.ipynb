{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7e12f-7623-4a4a-bcdf-8a49db93eb1c",
   "metadata": {},
   "source": [
    "# ASG wiki downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ceaad0-7843-406b-a9cb-954f22ead5d7",
   "metadata": {},
   "source": [
    "Downloads all grammar points articles from AllSet Learning Chinese Grammar Wiki (https://resources.allsetlearning.com/chinese/grammar/)\n",
    "\n",
    "* Saves each article's wiki markup to `wiki/<ID>.txt`.\n",
    "* List of article IDs and titles is saved to `wiki/index.tsv`. Titles from redirect pages's text.\n",
    "* Titles are also stored in a `<!-- -->` comment on the first line of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af76fda-75e5-49c9-9ad2-c27bf865c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, os, re, html, hashlib, time, urllib\n",
    "\n",
    "WIKI_DIR = '../wiki'\n",
    "CACHE_DIR = '../../data/asg/cache'\n",
    "PACE = 30    # delay between issuing http requests\n",
    "\n",
    "!mkdir -p {CACHE_DIR} {WIKI_DIR}\n",
    "\n",
    "def get_cached_or_download(url):\n",
    "    id = hashlib.sha256(url.encode('utf-8')).hexdigest()[:8]\n",
    "    cache_path = f'{CACHE_DIR}/{id}.html'\n",
    "    if os.path.exists(cache_path):\n",
    "        return open(cache_path).read()\n",
    "    print('Downloading %s' % url)\n",
    "    resp = requests.get(url, allow_redirects=True)\n",
    "    assert resp.ok\n",
    "    text = resp.content.decode('utf-8')\n",
    "    if '<html' in text.lower():\n",
    "        assert '</html>' in text.lower()\n",
    "    with open(cache_path, 'w') as f:\n",
    "        f.write(text)\n",
    "    print('OK')\n",
    "    time.sleep(PACE)\n",
    "    return text\n",
    "\n",
    "def get_wiki_source(page_id):\n",
    "    quoted = urllib.request.quote(page_id)\n",
    "    url = f'https://resources.allsetlearning.com/gramwiki/?title={quoted}&action=edit'\n",
    "    text = get_cached_or_download(url)\n",
    "    assert text.count('<textarea ') == 1, (url, text)\n",
    "    assert text.count('</textarea>') == 1\n",
    "    text = text[text.index('<textarea '):text.index('</textarea>')]\n",
    "    text = re.sub('^<textarea.* name=\"wpTextbox1\">', '', text)\n",
    "    assert '<textarea' not in text\n",
    "    assert '<' not in text\n",
    "    return html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f224fb42-c089-42de-b31a-30519f1b1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASG_TO_NAME = {}\n",
    "\n",
    "def visit_asg_page(asg_id):\n",
    "    assert asg_id.startswith('ASG')\n",
    "\n",
    "    text = get_wiki_source(asg_id)\n",
    "    m = re.match(r'#REDIRECT *\\[\\[(.*)\\]\\]', text.strip())\n",
    "    assert m\n",
    "    redir_id = m[1].strip()\n",
    "    redir_text = text\n",
    "    text = get_wiki_source(redir_id)\n",
    "\n",
    "    assert re.search(r'grammar point\\|\\s*%s' % asg_id, text) or asg_id in [\n",
    "        'ASGE8810' # used to have {{Basic grammar}}, but intentionally pulled of grammar lists, still has ID and examples\n",
    "    ], (asg_id, text)\n",
    "    assert '{{Grammar' in text or asg_id in [\n",
    "        'ASGE8810',\n",
    "        'ASGBA782','ASG23903', 'ASG87B11',   #stubs\n",
    "    ], (asg_id, text)\n",
    "\n",
    "    assert redir_id != asg_id\n",
    "    assert not redir_id.startswith('ASG')\n",
    "    assert '\\n' not in redir_id\n",
    "    assert redir_id == redir_id.strip()\n",
    "    title = redir_id\n",
    "\n",
    "    with open(f'{WIKI_DIR}/{asg_id}.txt', 'w') as f:\n",
    "        f.write(f'<!-- {title} -->\\n{text}')\n",
    "\n",
    "    ASG_TO_NAME.setdefault(asg_id, title)\n",
    "    assert ASG_TO_NAME[asg_id] == title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b0ee5f-1914-44c2-9a89-fc5ba7b725da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://resources.allsetlearning.com/chinese/grammar/A1_grammar_points found 40 expected 40\n",
      "https://resources.allsetlearning.com/chinese/grammar/A2_grammar_points found 99 expected 99\n",
      "https://resources.allsetlearning.com/chinese/grammar/B1_grammar_points found 143 expected 143\n",
      "https://resources.allsetlearning.com/chinese/grammar/B2_grammar_points found 152 expected 154\n",
      "https://resources.allsetlearning.com/chinese/grammar/C1_grammar_points found 61 expected 69\n",
      "https://resources.allsetlearning.com/chinese/grammar/HSK_1_grammar_points found 54 expected 54\n",
      "https://resources.allsetlearning.com/chinese/grammar/HSK_2_grammar_points found 78 expected 79\n",
      "https://resources.allsetlearning.com/chinese/grammar/HSK_3_grammar_points found 86 expected 87\n",
      "https://resources.allsetlearning.com/chinese/grammar/HSK_4_grammar_points found 113 expected 115\n",
      "https://resources.allsetlearning.com/chinese/grammar/HSK_5_grammar_points found 107 expected 109\n",
      "https://resources.allsetlearning.com/chinese/grammar/HSK_6_grammar_points found 59 expected 66\n",
      "Total 496 unique IDs\n"
     ]
    }
   ],
   "source": [
    "ids = set()\n",
    "for url in [\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/A1_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/A2_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/B1_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/B2_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/C1_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/HSK_1_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/HSK_2_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/HSK_3_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/HSK_4_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/HSK_5_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/HSK_6_grammar_points',\n",
    "    ]:\n",
    "    text = get_cached_or_download(url)\n",
    "    m = re.search('There are <b>([0-9]+)</b> total.{1,20}grammar points', text)\n",
    "    assert m, url\n",
    "    kexpect = int(m[1])\n",
    "    kfound = 0\n",
    "    for link in re.findall('<a [^>]*href=\"([^\"]+)\"', text):\n",
    "        m = re.match('/chinese/grammar/(ASG.*)', link)\n",
    "        if m:\n",
    "            assert len(m[1]) == 8\n",
    "            kfound += 1\n",
    "            ids.add(m[1])\n",
    "    print(f'{url} found {kfound} expected {kexpect}')\n",
    "\n",
    "print('Total %d unique IDs' % len(ids))\n",
    "for page_id in sorted(set(ids)):\n",
    "    visit_asg_page(page_id)\n",
    "\n",
    "# some discrepancies because of stubs / incomplete articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ed3f56-8e6d-4753-9107-3dc33a35aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521 links\n",
      "Missing ID: \"Prefer...rather_than\"_as_\"与其......宁可\" {{stub}}\n",
      "Missing ID: Additional_way_to_express_\"in_the_name_of\" {{stub}}\n",
      "Missing ID: Advanced_uses_of_\"bei\" {{stub}}\n",
      "Missing ID: After_or_before_a_specific_time\n",
      "Missing ID: Ba-test\n",
      "Missing ID: Comparing_\"jingli\"_and_\"tiyan\" {{stub}}\n",
      "Missing ID: Comparing_\"tuiguang\"_and_“xuanchuan\" {{stub}}\n",
      "Missing ID: Express_an_action_and_its_effect_by_using_\"tongguo…_shi\" {{stub}}\n",
      "Missing ID: Expressing_\"age_difference\"_with_\"da_and_xiao\"\n",
      "Missing ID: Expressing_\"however\"_with_\"ran'er\"\n",
      "Missing ID: Expressing_\"inevitably\"_with_\"shibi\" {{stub}}\n",
      "Missing ID: Expressing_uncertainty_about_\"whether_or_not\" {{stub}}\n",
      "Missing ID: Rhetorical_questions {{stub}}\n",
      "Missing ID: Taiwanese_Mandarin_use_of_\"bucuo\" {{stub}}\n",
      "Missing ID: Use_\"tangruo\"_to_express_\"if\" {{stub}}\n",
      "Missing ID: Using_the_\"suo\"_structure {{stub}}\n"
     ]
    }
   ],
   "source": [
    "# Find the rest of articles via 'what links here' and category lists\n",
    "\n",
    "links = set()\n",
    "for url in [\n",
    "        'https://resources.allsetlearning.com/gramwiki/?title=Special:WhatLinksHere/Template:Grammar_Box&limit=500',\n",
    "        'https://resources.allsetlearning.com/gramwiki/?title=Special:WhatLinksHere/Template:Grammar_Box&limit=500&from=4166&back=0',\n",
    "        'https://resources.allsetlearning.com/gramwiki/?title=Special:WhatLinksHere/Template:HSK&limit=500',\n",
    "        'https://resources.allsetlearning.com/gramwiki/?title=Special:WhatLinksHere/Template:Structure&limit=500',\n",
    "        'https://resources.allsetlearning.com/gramwiki/?title=Special:WhatLinksHere/Template:Used_for&limit=500',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/Category:A1_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/Category:A2_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/Category:B1_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/Category:B2_grammar_points',\n",
    "        'https://resources.allsetlearning.com/chinese/grammar/Category:C1_grammar_points',\n",
    "        #'https://resources.allsetlearning.com/gramwiki/?title=Special:ListRedirects&limit=500&offset=0',\n",
    "        #'https://resources.allsetlearning.com/gramwiki/?title=Special:ListRedirects&limit=500&offset=500',\n",
    "        #'https://resources.allsetlearning.com/gramwiki/?title=Special:ListRedirects&limit=500&offset=1000',\n",
    "        #'https://resources.allsetlearning.com/gramwiki/?title=Special:ListRedirects&limit=500&offset=1500',\n",
    "    ]:\n",
    "    text = get_cached_or_download(url)\n",
    "    for link in re.findall('<a [^>]*href=\"([^\"]+)\"', text):\n",
    "        if not link.startswith('/chinese/grammar/'): continue\n",
    "        if re.match('.*/(Special:.*|Chinese_Grammar_Wiki:.*|Category:.*|Template:.*|File:.*[.]jpg$)$', link):continue\n",
    "        link = link[len('/chinese/grammar/'):]\n",
    "        link = urllib.request.unquote(link)\n",
    "        links.add(link)\n",
    "\n",
    "links = set(links) - set('''\n",
    "A1_grammar_points\n",
    "A2_grammar_points\n",
    "Acknowledgments\n",
    "B1_grammar_points\n",
    "B2_book_omissions\n",
    "B2_grammar_points\n",
    "C1_grammar_points\n",
    "Chinese_textbook_grammar_index\n",
    "Contact\n",
    "Forums\n",
    "Grammar_points_by_level\n",
    "HSK_1_grammar_points\n",
    "HSK_2_grammar_points\n",
    "HSK_3_grammar_points\n",
    "Interrogative_pronouns\n",
    "Keywords\n",
    "Learner_FAQ\n",
    "Main_Page\n",
    "Measure_word\n",
    "Result_complement\n",
    "State_complement\n",
    "Tools\n",
    "Translations\n",
    "'''.strip().split())\n",
    "\n",
    "print('%d links' % len(links))\n",
    "\n",
    "for page_id in sorted(set(links)):\n",
    "    text = get_wiki_source(page_id)\n",
    "    m = re.search(r'\\|grammar point\\|\\s*(ASG.....)}}', text)\n",
    "    if not m:\n",
    "        if text.strip().startswith('#REDIRECT'): continue\n",
    "        print('Missing ID: %s%s' % (page_id, ' {{stub}}' if '{{stub}}' in text.lower() else ''))\n",
    "        #print('\\t', repr(text)[:min(len(text), 500)])\n",
    "        continue\n",
    "    asg_id = m[1]\n",
    "    visit_asg_page(asg_id)\n",
    "\n",
    "visit_asg_page('ASGE8810')  # missing main grammar templates to make discoverable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8681fca-d355-4079-ad89-4cb0a4748c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506 total grammar point articles downloaded\n"
     ]
    }
   ],
   "source": [
    "asg_df = pd.DataFrame(ASG_TO_NAME.items(), columns=['ID', 'Title']).sort_values('ID')\n",
    "asg_df.to_csv(f'{WIKI_DIR}/index.tsv', sep='\\t', index=False)\n",
    "print('%d total grammar point articles downloaded' % len(asg_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2ffd76-d955-483d-bc30-0a3c77a7893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 24526 Oct 14 15:13 ../wiki/index.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls -l {WIKI_DIR}/index.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
